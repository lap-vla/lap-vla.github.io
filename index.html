<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LAP: Language-Action Pre-training Enables Zero-Shot Cross-Embodiment Transfer">
  <meta name="keywords"
    content="LAP, Language-Action Pre-training, Vision-Language-Action, Robotics, Cross-Embodiment Transfer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LAP: Language-Action Pre-training Enables Zero-Shot Cross-Embodiment Transfer</title>

  <link href="https://fonts.googleapis.com/css2?family=Google+Sans|Noto+Sans|Castoro|Inter:wght@400;500;600;700" rel="stylesheet">
  <link rel="icon" href="./favicon.svg" type="image/png" />

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']]
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
</head>

<body>
  <!-- Progress Sidebar -->
  <nav class="progress-sidebar" aria-label="Page progress">
    <div class="progress-track">
      <div class="progress-line"></div>
      <div class="progress-line-fill"></div>
      <div class="progress-item" data-section="hero">
        <a href="#">
          <span class="progress-dot"></span>
          <span class="progress-label">Overview</span>
        </a>
      </div>
      <div class="progress-item" data-section="zeroshot">
        <a href="#zeroshot">
          <span class="progress-dot"></span>
          <span class="progress-label">Zero-shot</span>
        </a>
      </div>
      <div class="progress-item" data-section="finetuning">
        <a href="#finetuning">
          <span class="progress-dot"></span>
          <span class="progress-label">Fine-tuning</span>
        </a>
      </div>
      <div class="progress-item" data-section="analysis">
        <a href="#analysis">
          <span class="progress-dot"></span>
          <span class="progress-label">Analysis</span>
        </a>
      </div>

      <div class="progress-item" data-section="benefits">
        <a href="#benefits">
          <span class="progress-dot"></span>
          <span class="progress-label">Beyond Cross-Embodiment</span>
        </a>
      </div>
    </div>
  </nav>

  <section class="teaser">
    <div class="hero-header">
      <div class="hero-header-inner">
        <div class="hero-header-content">
          <h1 class="publication-title" style="max-width: 1020px; margin: 0 auto; font-size: clamp(26.6px, 3.8vw, 53.2px); line-height: 1.2;">
            <span style="color: #d84f00; font-weight: 900; letter-spacing: 0.01em;">LAP:</span> Language-Action Pre-training Enables Zero-Shot Cross-Embodiment Transfer
          </h1>
          <div class="authors" style="margin: 0 auto 15px auto;">
            <a href="https://lihzha.github.io/" target="_blank">Lihan Zha</a><sup>1</sup>,
            <a href="https://aasherh.github.io/" target="_blank">Asher J. Hancock</a><sup>1</sup>*,
            <a href="https://robo-alex.github.io/" target="_blank">Mingtong Zhang</a><sup>1</sup>*,
            <a href="https://tenny-yinyijun.github.io/" target="_blank">Tenny Yin</a><sup>1</sup>,
            <a href="https://yixuanhuang98.github.io/" target="_blank">Yixuan Huang</a><sup>1</sup>,<br>
            <a href="https://robodhruv.github.io/" target="_blank">Dhruv Shah</a><sup>1</sup>,
            <a href="https://allenzren.github.io/" target="_blank">Allen Z. Ren</a><sup>2</sup><sup>&dagger;</sup>,
            <a href="https://irom-lab.princeton.edu/majumdar" target="_blank">Anirudha Majumdar</a><sup>1</sup><sup>&dagger;</sup>
            <span class="author-note">*Equal contribution, <sup>&dagger;</sup>Equal advising</span>
            <span class="affiliation">
              <sup>1</sup>Princeton University, <sup>2</sup>Physical Intelligence
            </span>
          </div>
          <div class="publication-links header-links">
            <a class="button is-rounded" href="https://arxiv.org/abs/2602.10556" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
            <a class="button is-rounded" href="https://github.com/lihzha/lap" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <a class="button is-rounded" href="https://huggingface.co/collections/lihzha/lap" target="_blank">
              <span class="icon"><i class="fas fa-download"></i></span>
              <span>Checkpoints</span>
            </a>
          </div>
        </div>
      </div>
      
      <div class="hero-waves" aria-hidden="true">
        <svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
          viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
          <defs>
            <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path>
          </defs>
          <g class="wave-parallax">
            <use xlink:href="#gentle-wave" x="48" y="5" fill="rgba(240,130,50,0.25)"></use>
            <use xlink:href="#gentle-wave" x="48" y="-5" fill="rgba(255,215,0,0.2)"></use>
            <use xlink:href="#gentle-wave" x="48" y="3" fill="rgba(134,184,231,0.1)"></use>
            <!-- <use xlink:href="#gentle-wave" x="48" y="5" fill="rgba(255,127,80,0.18)"></use> -->
            <use xlink:href="#gentle-wave" x="48" y="8" fill="rgba(240,120,0,0.35)"></use>
          </g>
        </svg>
      </div>
    </div>

    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <video autoplay muted controls loop playsinline style="width: 100%; height: auto; border-radius: 12px;">
              <source src="./videos/teaser.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p class="figure-caption" style="width: 95%; margin: 1rem auto 0 auto; text-align: center; font-size: 1.1rem; line-height: 1.55; color: #3a3a3a;">
              We introduce
              <span style="font-weight: 900; color: #EC5800;">LAP-3B</span>,
              the first VLA that achive substantial
              <span style="font-weight: 900; color: #0B6E99; background: linear-gradient(transparent 62%, rgba(11,110,153,0.18) 62%); padding: 0 0.18em; border-radius: 4px;">zero-shot generalization</span>
              to
              <span style="font-weight: 800; color: #2B7A3D;">unseen embodiments</span>.
            </p>
            <!-- <p>
              A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which demonstrates substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.
            </p> -->
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -2.0rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Overview</h2>
          <div class="item item-steve" style="display: flex; flex-direction: column; align-items: center;">
            <img src="./images/fig1_banner.png" alt="LAP Overview" style="max-width: 100%; width: 100%; height: auto;">
          </div>
          <p class="figure-caption" style="width: 95%;">
            We introduce Language-Action Pre-training (LAP), a general VLA pre-training recipe that represents low-level actions in natural language to supervise a vision-language backbone, unifying action learning and VQA. We instantiate this approach as LAP-3B, the first VLA to demonstrate strong zero-shot transfer to novel embodiments. Compared to state-of-the-art VLAs, LAP-3B learns more generalizable embodiment representations and exhibits favorable scaling behavior.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3" id="zeroshot">Zero-shot Cross-Embodiment Generalization</h2>
          <!-- <div class="content has-text-justified">
            <p>
              LAP-3B achieves performance comparable to \(\pi_{0.5}\)-DROID on the seen embodiment. Across three previously unseen embodiments and six real-world manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering approximately a 2x improvement over the strongest baselines. Error bars denote 95% statistically valid finite-sample confidence intervals.
            </p>
          </div> -->
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -2.0rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="item item-steve" style="display: flex; flex-direction: column; align-items: center;">
            <img src="./images/fig3_zeroshot.png" alt="Zero-shot cross-embodiment generalization performance" style="max-width: 100%; width: 100%; height: auto;">
          </div>
          <p class="figure-caption" style="width: 95%;">
            <strong>LAP-3B achieves over 50% average success when deployed zero-shot on previously unseen robot embodiments</strong>,
            delivering roughly a <strong>2x improvement</strong> over the strongest prior baseline.
            In contrast, all other open-source VLAs fail to transfer, collapsing to <strong>0% success</strong>
            under the same evaluation.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-4">Zero-shot Rollouts (LAP-3B)</h3>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section" style="padding-top: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">

          <div id="zeroshot-yam" class="zeroshot-robot-section">
            <h4 class="title is-5">YAM</h4>
            <div class="video-grid three-col">
              <div class="video-card">
                <p class="video-label">"Sort things into the basket"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/yam/yam_sort_success_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Take out a tissue and put it on table"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/yam/yam_tissue_success_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Cover the carrot with towel"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/yam/yam_cover_4x.mp4"></video>
              </div>
            </div>
          </div>


          <div id="zeroshot-kinova" class="zeroshot-robot-section">
            <h4 class="title is-5">Kinova</h4>
            <div class="video-grid three-col">
              <div class="video-card">
                <p class="video-label">"Put towel into the basket"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/kinova/kinova_towel_success_1x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Put carrot into the basket"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/kinova/carrot_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Put marker into the bowl"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/kinova/kinova_marker_success_4x.mp4"></video>
              </div>
            </div>
          </div>
          <div id="zeroshot-franka" class="zeroshot-robot-section">
            <h4 class="title is-5">Custom Franka</h4>
            <div class="video-grid">
              <div class="video-card">
                <p class="video-label">"Sort things into the basket"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/franka/franka_sort_success_cropped_1x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Put the mug into the basket"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/franka/franka_mug_success_cropped_1x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Put carrot into the bowl"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/franka/franka_carrot_success_cropped_4x.mp4"></video>
              </div>
            </div>
          </div>
          <div id="zeroshot-droid" class="zeroshot-robot-section">
            <h4 class="title is-5">DROID (Seen)</h4>
            <div class="video-grid">
              <div class="video-card">
                <p class="video-label">"Put marker into the cup"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/droid/lap_marker_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Pour things onto the table"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/droid/lap_bowl_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label">"Put banana on the pan"</p>
                <video autoplay muted controls loop src="./videos/sec1_zeroshot_ours/droid/lap_banana_2x.mp4"></video>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -3rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h3 class="title is-4" id="comparison">Zero-Shot Comparison</h3> -->
          <div class="content has-text-justified">
            <p>
              We further compare LAP-3B against state-of-the-art open-sourced vision-language-action models‚Äîincluding \(\pi_0\), \(\pi_{0.5}\)-DROID, X-VLA, and \(\pi_{0.5}\)-replicated‚Äîon the same tasks across multiple embodiments under zero-shot transfer. The qualitative rollouts demonstrate LAP-3B's superior ability to generalize to novel robots without any embodiment-specific fine-tuning, while baseline policies struggle or fail entirely on these unseen platforms.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -2.0rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="dropdown-row">
            <div class="dropdown-group">
              <span class="dropdown-label">Comparison:</span>
              <select id="comparison-selection" class="dropdown-select" onchange="selectComparison()">
                <option value="franka" selected>Custom Franka</option>
                <option value="yam">YAM</option>
                <option value="kinova">Kinova</option>
                <option value="droid">DROID</option>
              </select>
            </div>
          </div>

          <div id="comparison-franka" class="comparison-section">
            <h4 class="title is-5" style="margin-top: 2rem;">Custom Franka: Sort</h4>
            <div class="video-grid four-col">
              <div class="video-card">
                <p class="video-label" style="text-align: center;">X-VLA ‚ùå</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/franka/xvla_toy_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0.5}\)-DROID ‚ùå</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/franka/pi05_droid_toy_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0.5}\)-replicated ‚ö†Ô∏è</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/franka/fast_toy_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center; color: #EC5800; font-weight: 900; font-size: 1.1rem;">LAP-3B ‚úÖ</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/franka/lap_toy_2x.mp4"></video>
              </div>
            </div>
          </div>

          <div id="comparison-yam" class="comparison-section" style="display: none;">
            <h4 class="title is-5" style="margin-top: 2rem;">YAM: Tissue</h4>
            <div class="video-grid three-col">
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0}\)-replicated ‚ö†Ô∏è</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/yam/pi0_tissue_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0.5}\)-replicated ‚ö†Ô∏è</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/yam/tissue_fail_pi05_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center; color: #EC5800; font-weight: 900; font-size: 1.1rem;">LAP-3B ‚úÖ</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/yam/lap_tissue_2x.mp4"></video>
              </div>
            </div>
          </div>

          <div id="comparison-kinova" class="comparison-section" style="display: none;">
            <h4 class="title is-5" style="margin-top: 2rem;">Kinova: Marker</h4>
            <div class="video-grid three-col">
              <div class="video-card">
                <p class="video-label" style="text-align: center;">X-VLA ‚ùå</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/kinova/xvla_marker_4x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0.5}\)-DROID ‚ùå</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/kinova/pi05droid_marker_4x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center; color: #EC5800; font-weight: 900; font-size: 1.1rem;">LAP-3B ‚úÖ</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/kinova/lap_marker_4x.mp4"></video>
              </div>
            </div>
          </div>

          <div id="comparison-droid" class="comparison-section" style="display: none;">
            <h4 class="title is-5">DROID: Pour</h4>
            <div class="video-grid four-col">
              <div class="video-card">
                <p class="video-label" style="text-align: center;">X-VLA ‚ùå</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/droid/xvla_bowl_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0.5}\)-DROID ‚ö†Ô∏è</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/droid/pi05_droid_bowl_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center;">\(\pi_{0.5}\)-replicated ‚ö†Ô∏è</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/droid/fast_bowl_2x.mp4"></video>
              </div>
              <div class="video-card">
                <p class="video-label" style="text-align: center; color: #EC5800; font-weight: 900; font-size: 1.1rem;">LAP-3B ‚úÖ</p>
                <video autoplay muted controls loop src="./videos/sec2_zeroshot_comparison/droid/lap_bowl_2x.mp4"></video>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3" id="finetuning">Superior Fine-tuning Efficiency</h2>
          <div class="content has-text-justified">
            <!-- <p>
              Across both simulation and real-world manipulation tasks, LAP-3B adapts substantially faster than baselines, reaching high performance with significantly fewer epochs and demonstrations.
            </p> -->
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -2.0rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="item item-steve" style="display: flex; flex-direction: column; align-items: center;">
            <img src="./images/fig4_finetune.png" alt="Fine-tuning efficiency" style="max-width: 100%; width: 100%; height: auto;">
          </div>
          <p class="figure-caption" style="width: 95%;">
            LAP-3B fine-tunes more efficiently than baseline policies across both the LIBERO benchmark and real-world manipulation. On LIBERO, <span class="smallcaps">LAP-3B</span> reaches near-optimal success with only a fraction of the training steps required by prior methods. On real robots, it achieves comparable performance using approximately <strong>2.5x fewer demonstrations</strong>, demonstrating substantially improved data and compute efficiency when transferring to new embodiments.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="padding-top: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="video-grid two-col">
            <div class="video-card">
              <p class="video-label">YAM: Hang Tape on Rack</p>
              <video autoplay muted controls loop src="./videos/sec3_finetune/yam_ft_tape_success_1x.mp4"></video>
            </div>
            <div class="video-card">
              <p class="video-label">Franka: Fold Towel and Place in Basket</p>
              <video autoplay muted controls loop src="./videos/sec3_finetune/franka_ft_towel_success_cropped_2x.mp4"></video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3" id="analysis">Analysis of LAP's Cross-Embodiment Generalization</h2>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -2.0rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="item item-steve" style="display: flex; flex-direction: column; align-items: center;">
            <img src="./images/fig5_analysis.png" alt="Why LAP enables cross-embodiment generalization" style="max-width: 100%; width: 100%; height: auto;">
          </div>
          <p class="figure-caption" style="width: 95%; text-align: left;">
            <strong>Left: T-SNE visualizations of learned embodiment representations</strong> for <span class="smallcaps">LAP-3B</span> and \(\pi_{0.5}\)-replicated. <span class="smallcaps">LAP-3B</span> exhibits substantial overlap between training and unseen embodiments, whereas \(\pi_{0.5}\)-replicated shows limited alignment, indicating that <span class="smallcaps">LAP-3B</span> learns more transferable, embodiment-agnostic control representations.
          </p>
          <p class="figure-caption" style="width: 95%; text-align: left;">
            <strong>Right: Action prediction error on unseen embodiments during pre-training.</strong> <span class="smallcaps">LAP-3B</span> achieves consistently lower action prediction error on held-out unseen embodiments throughout training, compared to \(\pi_{0.5}\)-replicated and \(\pi_{0}\)-replicated baselines. This indicates that language-action supervision enables the model to learn control representations that generalize across embodiments, allowing more accurate action prediction on novel robots as well as smoother training dynamics.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: 1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3" id="benefits">Beyond Cross-Embodiment: Additional Benefits of LAP</h2>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              <strong>Enhanced language following ability.</strong> While our primary focus is cross-embodiment generalization, we also demonstrate strong language-following capability by pre-training with language-actions. In cluttered scenes with multiple objects, the policy correctly identifies the instructed target and reliably completes the task.
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="video-grid three-col">
            <div class="video-card">
              <p class="video-label" style="text-align: center;">Carrot ü•ï</p>
              <video autoplay muted controls loop src="./videos/sec4_language_following/carrot_2x.mp4"></video>
            </div>
            <div class="video-card">
              <p class="video-label" style="text-align: center;">Corn üåΩ</p>
              <video autoplay muted controls loop src="./videos/sec4_language_following/corn_1x.mp4"></video>
            </div>
            <div class="video-card">
              <p class="video-label" style="text-align: center;">Grape üçá</p>
              <video autoplay muted controls loop src="./videos/sec4_language_following/grape_4x.mp4"></video>
            </div>
          </div>
          <div class="item item-steve" style="display: flex; flex-direction: column; align-items: center;">
            <img src="./images/fig6_benefits.png" alt="Additional benefits of LAP" style="max-width: 100%; width: 100%; height: auto;">
          </div>
          <p class="figure-caption" style="width: 95%; text-align: left;">
            <strong>Left: Better alignment with VQA co-training.</strong>
            Because language-actions share the same natural-language output space as standard VQA tasks, LAP enables seamless co-training with vision-language objectives. We introduce a motion-prediction task where the model describes the robot's movement between two frames using a language-action. This unified interface leads to more precise action generation and stronger spatial generalization across embodiments.
          </p>
          <p class="figure-caption" style="width: 95%; text-align: left;">
            <strong>Right: Favorable scaling with model size.</strong>
            LAP improves consistently as model capacity increases. Scaling from 4B to 27B parameters reduces both token and action validation losses, while comparable baselines saturate or degrade. These results demonstrate stable large-model scaling and increasing performance gains from additional capacity under language-action supervision.
          </p>
        </div>
      </div>
    </div>
  </section>

  <h2 class="title is-3 has-text-centered" id="bibtex">BibTeX</h2>
  <div class="bibtex-code">
    <div class="bibtex-title">Reference</div>
    <pre><code>@misc{zha2026laplanguageactionpretrainingenables,
  title={LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer},
  author={Lihan Zha and Asher J. Hancock and Mingtong Zhang and Tenny Yin and Yixuan Huang and Dhruv Shah and Allen Z. Ren and Anirudha Majumdar},
  year={2026},
  eprint={2602.10556},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2602.10556},
}</code></pre>
  </div>

  <p class="bibtex-footnote" style="text-align: center; font-size: 0.92rem; color: #6b7280;">
    This website is based on the <a href="https://github.com/polaris-evals/polaris-evals.github.io" target="_blank">PolaRiS template</a>.
  </p>


  <script>
    // Progress Sidebar
    document.addEventListener('DOMContentLoaded', () => {
      const progressItems = document.querySelectorAll('.progress-item');
      const progressLineFill = document.querySelector('.progress-line-fill');
      const progressTrack = document.querySelector('.progress-track');

      const sections = [];
      progressItems.forEach(item => {
        const id = item.dataset.section;
        let el;
        if (id === 'hero') {
          el = document.querySelector('.teaser');
        } else {
          el = document.getElementById(id);
        }
        if (el) {
          sections.push({ id, el });
        }
      });

      const updateProgress = () => {
        if (sections.length === 0) return;

        const viewportHeight = window.innerHeight;
        const activationThreshold = viewportHeight * 0.35;

        let activeIdx = 0;
        for (let idx = 0; idx < sections.length; idx++) {
          const rect = sections[idx].el.getBoundingClientRect();
          if (rect.top <= activationThreshold) {
            activeIdx = idx;
          } else {
            break;
          }
        }

        if (progressLineFill && progressTrack) {
          const trackHeight = progressTrack.offsetHeight;
          const maxFillHeight = trackHeight - 44;
          const fillHeight = (activeIdx / Math.max(sections.length - 1, 1)) * maxFillHeight;
          progressLineFill.style.height = `${Math.min(Math.max(fillHeight, 0), maxFillHeight)}px`;
        }

        progressItems.forEach(item => {
          const sectionId = item.dataset.section;
          const sectionIdx = sections.findIndex(s => s.id === sectionId);

          item.classList.remove('active', 'passed');
          if (sectionIdx === activeIdx) {
            item.classList.add('active');
          } else if (sectionIdx >= 0 && sectionIdx < activeIdx) {
            item.classList.add('passed');
          }
        });
      };

      const getVideoSpeedLabel = (videoSrc) => {
        if (!videoSrc) return '1x';
        const fileName = videoSrc.split('/').pop() || '';
        const baseName = fileName.split('.')[0] || '';
        const parts = baseName.split('_');
        const suffix = parts[parts.length - 1] || '';
        const match = suffix.match(/^(\d+)x$/i);
        return match ? `${match[1]}x` : '1x';
      };

      const enforceVideoMute = (video) => {
        video.muted = true;
        video.defaultMuted = true;
        video.volume = 0;
        video.setAttribute('muted', '');

        if (!video.dataset.muteLocked) {
          video.addEventListener('volumechange', () => {
            if (!video.muted || video.volume > 0) {
              video.muted = true;
              video.defaultMuted = true;
              video.volume = 0;
            }
          });
          video.dataset.muteLocked = 'true';
        }
      };

      const initializeVideoLoading = () => {
        const videos = document.querySelectorAll('video');
        const deferredVideos = [];

        const loadVideo = (video) => {
          if (video.dataset.loaded === 'true') return;

          const videoSrc = video.getAttribute('data-src');
          if (videoSrc) {
            video.setAttribute('src', videoSrc);
          }

          const sourceNodes = video.querySelectorAll('source[data-src]');
          sourceNodes.forEach(source => {
            const sourceSrc = source.getAttribute('data-src');
            if (sourceSrc) {
              source.setAttribute('src', sourceSrc);
            }
          });

          video.load();
          video.dataset.loaded = 'true';
        };

        videos.forEach(video => {
          enforceVideoMute(video);

          const isTeaserVideo = video.closest('.teaser') !== null;
          if (isTeaserVideo) {
            video.setAttribute('preload', 'metadata');
            return;
          }

          const directSrc = video.getAttribute('src');
          if (directSrc) {
            video.setAttribute('data-src', directSrc);
            video.removeAttribute('src');
          }

          const sourceNodes = video.querySelectorAll('source[src]');
          sourceNodes.forEach(source => {
            source.setAttribute('data-src', source.getAttribute('src'));
            source.removeAttribute('src');
          });

          video.setAttribute('preload', 'none');
          deferredVideos.push(video);
        });

        if (deferredVideos.length === 0) return;

        if (!('IntersectionObserver' in window)) {
          deferredVideos.forEach(loadVideo);
          return;
        }

        const observer = new IntersectionObserver((entries) => {
          entries.forEach(entry => {
            const video = entry.target;
            if (entry.isIntersecting) {
              loadVideo(video);
              if (video.hasAttribute('autoplay')) {
                video.play().catch(() => {});
              }
            } else if (video.hasAttribute('autoplay')) {
              video.pause();
            }
          });
        }, { rootMargin: '250px 0px' });

        deferredVideos.forEach(video => observer.observe(video));
      };

      const addVideoSpeedBadges = () => {
        const videos = document.querySelectorAll('.video-card video');
        videos.forEach(video => {
          if (video.parentElement && video.parentElement.classList.contains('video-with-speed')) {
            return;
          }
          const wrapper = document.createElement('div');
          wrapper.className = 'video-with-speed';
          video.parentNode.insertBefore(wrapper, video);
          wrapper.appendChild(video);

          const badge = document.createElement('span');
          badge.className = 'video-speed-badge';
          badge.textContent = getVideoSpeedLabel(video.getAttribute('data-src') || video.getAttribute('src'));
          wrapper.appendChild(badge);
        });
      };

      initializeVideoLoading();
      addVideoSpeedBadges();
      window.addEventListener('scroll', updateProgress, { passive: true });
      updateProgress();
    });

    function selectComparison() {
      var selected = document.getElementById('comparison-selection').value;
      var sections = document.querySelectorAll('.comparison-section');
      sections.forEach(function (section) {
        section.style.display = 'none';
      });
      var active = document.getElementById('comparison-' + selected);
      if (active) {
        active.style.display = 'block';
      } 
    }
  </script>
</body>

</html>
